{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/m_azimi/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/m_azimi/SHA_Diagonal/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stsb-uniqu: [91.8, 91.2, 92.1, 91, 91.5] -> 91.5\n",
    "cola-unique: [67.9, 64.8, 67.9, 64.3, 68.6] -> 66.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roberta-base:\n",
    "\n",
    "MRPC = [89.71, 88.24, 88.24, 88.24, 89.22]\n",
    "\n",
    "RTE = [79.71, 78.26, 78.26,77.54, 77.53]\n",
    "\n",
    "STSB = [90.1, 90.3, 89.85, 90.1, 89.9]\n",
    "\n",
    "roberta-large:\n",
    "\n",
    "STSB = [91.9, 92.1, 91.9, 91.9, 91.8]\n",
    "\n",
    "CoLA = [69.3, 69.3, 64.5, 65.3, 67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/m_azimi/.cache/huggingface'\n",
    "os.environ['HF_LOCK_DIR'] = '/home/m_azimi/.cache/huggingface/locks'\n",
    "\n",
    "import config, GLUE_data_setup, peft_module, engine, utils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import warnings\n",
    "\n",
    "best_checkpoint_path = \"/home/m_azimi/SHA_Diagonal/best_model_checkpoint.pt\"\n",
    "\n",
    "\n",
    "num_labels = 3 if config.CONFIG.task.startswith(\"mnli\") else 1 if config.CONFIG.task == \"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.CONFIG.model_name, num_labels=num_labels, output_attentions=False,\n",
    "    output_hidden_states=False, cache_dir=\"/home/m_azimi/.cache/huggingface\"\n",
    ").to(config.CONFIG.device)\n",
    "\n",
    "for layer in model.roberta.encoder.layer:\n",
    "    #Create a new instance of your custom attention class with the same config and is_cross_attention settings\n",
    "    custom_attn = peft_module.CustomRobertaSelfAttention(model.config)\n",
    "    #Copy the weights from the original attention to the new custom attention\n",
    "    custom_attn.load_state_dict(layer.attention.self.state_dict())\n",
    "    #Replace the original attention with the custom one\n",
    "    layer.attention.self = custom_attn\n",
    "\n",
    "# Apply PEFT layers\n",
    "peft_module.add_peft_layers(model=model) \n",
    "peft_module.freeze_model(model)\n",
    "\n",
    "\n",
    "test_dataset = torch.load(\"/home/m_azimi/SHA_Diagonal/saved_data/test_dataset.pt\")\n",
    "\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config.CONFIG.valid_batch,\n",
    "    num_workers=1, shuffle=False, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Optionally, load the best checkpoint and perform additional evaluations\n",
    "model.load_state_dict(torch.load(best_checkpoint_path))\n",
    "model.to(config.CONFIG.device)\n",
    "final_test_loss, final_test_preds, final_test_true = engine.evaluate(model, test_loader)\n",
    "final_test_metric = engine.eval_func(final_test_preds, final_test_true)\n",
    "print(f'Final Test Metric: {final_test_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import config\n",
    "\n",
    "class SHA_DIAGONAL(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        r: int = config.CONFIG.r,\n",
    "        lora_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "\n",
    "        # recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)\n",
    "        self.pretrained = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.pretrained.weight.requires_grad = False\n",
    "\n",
    "        # create the down projection matrix and initialize with same method as in Hugging Face PEFT library\n",
    "        self.down_proj = nn.Linear(in_dim, r, bias=False)\n",
    "        self._kaiming_init(self.down_proj.weight, generator=torch.manual_seed(config.CONFIG.seed))\n",
    "\n",
    "        # Add the custom DiagonalLinear layer\n",
    "        self.diagonal_d = nn.Parameter(torch.ones(r), requires_grad=True)\n",
    "        nn.init.constant_(self.diagonal_d, 0.1)\n",
    "\n",
    "        # create the up projection matrix and initialize to zero\n",
    "        self.up_proj = nn.Linear(r, out_dim, bias=False)\n",
    "        self._kaiming_init(self.up_proj.weight, generator=torch.manual_seed(config.CONFIG.seed))\n",
    "\n",
    "        # Add the custom DiagonalLinear layer\n",
    "        self.diagonal_b = nn.Parameter(torch.zeros(out_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        pretrained_out = self.pretrained(x)\n",
    "\n",
    "        x_out = self.lora_dropout(x)\n",
    "\n",
    "        down_project_out = self.down_proj(x_out)\n",
    "\n",
    "        diagonal_d_out = down_project_out * self.diagonal_d\n",
    "\n",
    "        up_project_out = self.up_proj(diagonal_d_out)\n",
    "\n",
    "        diagonal_b_out = up_project_out * self.diagonal_b\n",
    "\n",
    "        return pretrained_out + diagonal_b_out\n",
    "\n",
    "\n",
    "    def _kaiming_init(self, tensor: torch.Tensor, generator: torch.Generator):\n",
    "\n",
    "        fan = nn.init._calculate_correct_fan(tensor, mode=\"fan_in\")\n",
    "        gain = math.sqrt(2.0)\n",
    "        std = gain / math.sqrt(fan)\n",
    "        bound = math.sqrt(3.0) * std\n",
    "        with torch.no_grad():\n",
    "            return tensor.uniform_(-bound, bound, generator=generator)        \n",
    "\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"diagonal_d\" not in name and \"diagonal_b\" not in name and \"classifier\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def create_peft(module):\n",
    "    \"\"\"Converts a linear module to a peft linear module.\"\"\"\n",
    "    k, d = module.weight.shape  # pytorch nn.Linear weights are transposed, that is why shape is (k, d) and not (d, k)\n",
    "    peft = SHA_DIAGONAL(in_dim=d, out_dim=k)\n",
    "    with torch.no_grad():\n",
    "        peft.pretrained.weight.copy_(module.weight)\n",
    "        peft.pretrained.bias.copy_(module.bias)\n",
    "    return peft   \n",
    "\n",
    "\n",
    "\n",
    "def add_peft_layers(\n",
    "    model,\n",
    "    module_names: Tuple=(\"query\", \"value\"),\n",
    "    ignore_layers: List[int]=[]\n",
    "):\n",
    "    module_types: Tuple=(nn.Linear,)\n",
    "\n",
    "    # disable dropout in frozen layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = 0.0\n",
    "    # replace chosen linear modules with lora modules\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, module_types) and name in module_names:\n",
    "            temp_peft = create_peft(module)\n",
    "            setattr(model, name, temp_peft)\n",
    "        else:\n",
    "            ignore_layers_str = [str(i) for i in ignore_layers]\n",
    "            if name not in ignore_layers_str:\n",
    "                add_peft_layers(module, module_names, ignore_layers)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHA_DIAGONAL(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        lora_alpha: int = 8,\n",
    "        lora_dropout: float = 0.1,\n",
    "        r: int = config.CONFIG.r,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "\n",
    "        # recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)\n",
    "        self.pretrained = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.pretrained.weight.requires_grad = False\n",
    "\n",
    "        # create the down projection matrix and initialize with same method as in Hugging Face PEFT library\n",
    "        self.down_proj = nn.Linear(in_dim, r, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.down_proj.weight, a = math.sqrt(5))\n",
    "\n",
    "        # create the up projection matrix and initialize to zero\n",
    "        self.up_proj = nn.Linear(r, out_dim, bias=False)\n",
    "        nn.init.constant_(self.up_proj.weight, 0)\n",
    "\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        pretrained_out = self.pretrained(x)\n",
    "\n",
    "        x_out = self.lora_dropout(x)\n",
    "\n",
    "        down_project_out = self.down_proj(x_out)\n",
    "\n",
    "        up_project_out = self.up_proj(down_project_out)\n",
    "\n",
    "        up_project_out = up_project_out * self.scaling\n",
    "\n",
    "        return pretrained_out + up_project_out\n",
    "\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"down_proj\" not in name and \"up_proj\" not in name and \"classifier\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "def create_peft(module):\n",
    "    \"\"\"Converts a linear module to a peft linear module.\"\"\"\n",
    "    k, d = module.weight.shape  # pytorch nn.Linear weights are transposed, that is why shape is (k, d) and not (d, k)\n",
    "    peft = SHA_DIAGONAL(in_dim=d, out_dim=k)\n",
    "    with torch.no_grad():\n",
    "        peft.pretrained.weight.copy_(module.weight)\n",
    "        peft.pretrained.bias.copy_(module.bias)\n",
    "    return peft  \n",
    "\n",
    "\n",
    "\n",
    "def add_peft_layers(\n",
    "    model,\n",
    "    module_names: Tuple=(\"query\", \"value\"),\n",
    "    ignore_layers: List[int]=[]\n",
    "):\n",
    "    module_types: Tuple=(nn.Linear,)\n",
    "\n",
    "    # disable dropout in frozen layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = 0.0\n",
    "    # replace chosen linear modules with lora modules\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, module_types) and name in module_names:\n",
    "            temp_peft = create_peft(module)\n",
    "            setattr(model, name, temp_peft)\n",
    "        else:\n",
    "            ignore_layers_str = [str(i) for i in ignore_layers]\n",
    "            if name not in ignore_layers_str:\n",
    "                add_peft_layers(module, module_names, ignore_layers)                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
