{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SHA_Diagonal/config.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class CONFIG:\n",
    "    output_dir = \"SHA-DIAG\"\n",
    "    task = \"rte\"\n",
    "    seed = 42\n",
    "    max_len = 128\n",
    "    train_batch = 32\n",
    "    valid_batch = 32\n",
    "    epochs = 40\n",
    "    learning_rate = 1e-2\n",
    "    classifier_learning_rate = 1e-2  # Different learning rate for the classifier head\n",
    "    warmup_ratio = 0.06\n",
    "    model_name = \"FacebookAI/roberta-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SHA_Diagonal/GLUE_data_setup.py\n",
    "\n",
    "import config\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "class GLUEDataset(Dataset):\n",
    "    def __init__(self, dataset_name=config.CONFIG.task, split=\"train\", tokenizer_name=config.CONFIG.model_name, max_len=config.CONFIG.max_len):\n",
    "        self.dataset_name = dataset_name\n",
    "        if self.dataset_name == \"sst2\":\n",
    "            self.dataset = load_dataset(dataset_name)[split].to_pandas()\n",
    "        else:    \n",
    "            self.dataset = load_dataset('glue', dataset_name)[split].to_pandas()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        if dataset_name in ['sst2', 'cola']:\n",
    "            self.text = self.dataset['sentence'].values\n",
    "            self.labels = self.dataset['label'].values\n",
    "        elif dataset_name in ['mrpc', 'qqp', 'stsb', 'rte']:\n",
    "            self.sentence1 = self.dataset['sentence1'].values\n",
    "            self.sentence2 = self.dataset['sentence2'].values\n",
    "            self.labels = self.dataset['label'].values\n",
    "        elif dataset_name == 'mnli':\n",
    "            self.premises = self.dataset['premise'].values\n",
    "            self.hypotheses = self.dataset['hypothesis'].values\n",
    "            self.labels = self.dataset['label'].values\n",
    "        elif dataset_name == 'qnli':\n",
    "            self.questions = self.dataset['question'].values\n",
    "            self.sentences = self.dataset['sentence'].values\n",
    "            self.labels = self.dataset['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataset_name in ['sst2', 'cola']:\n",
    "            text = self.text[index]\n",
    "            text = ' '.join(text.split())\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                None,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "        elif self.dataset_name in ['mrpc', 'qqp', 'stsb', 'rte']:\n",
    "            sentence1 = self.sentence1[index]\n",
    "            sentence2 = self.sentence2[index]\n",
    "            sentence1 = ' '.join(sentence1.split())\n",
    "            sentence2 = ' '.join(sentence2.split())\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                sentence1,\n",
    "                sentence2,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "        elif self.dataset_name == 'mnli':\n",
    "            premise = self.premises[index]\n",
    "            hypothesis = self.hypotheses[index]\n",
    "            premise = ' '.join(premise.split())\n",
    "            hypothesis = ' '.join(hypothesis.split())\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                premise,\n",
    "                hypothesis,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "        elif self.dataset_name == 'qnli':\n",
    "            question = self.questions[index]\n",
    "            sentence = self.sentences[index]\n",
    "            question = ' '.join(question.split())\n",
    "            sentence = ' '.join(sentence.split())\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                question,\n",
    "                sentence,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "\n",
    "        inputs['input_ids'] = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "        inputs['attention_mask'] = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            inputs['token_type_ids'] = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
    "\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long if self.dataset_name != 'stsb' else torch.float)\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": inputs['input_ids'],\n",
    "            \"attention_mask\": inputs['attention_mask'],\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            result[\"token_type_ids\"] = inputs['token_type_ids']\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SHA_Diagonal/peft_module.py\n",
    "\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class SHA_DIAGONAL(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        r: int = 24,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "\n",
    "        # recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)\n",
    "        self.pretrained = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.pretrained.weight.requires_grad = False\n",
    "\n",
    "        # create the down projection matrix and initialize with same method as in Hugging Face PEFT library\n",
    "        self.down_proj = nn.Linear(in_dim, r, bias=False)\n",
    "        #nn.init.kaiming_uniform_(self.down_proj.weight, a=math.sqrt(5))\n",
    "        \n",
    "        self.Wqkv = nn.Linear(r, (r // 4)*3, bias=False)\n",
    "        #nn.init.kaiming_uniform_(self.Wqkv.weight, a=math.sqrt(1))\n",
    "        \n",
    "        self.Wo = nn.Linear(r // 4, r, bias=False)\n",
    "        #nn.init.kaiming_uniform_(self.Wo.weight, a=math.sqrt(2))\n",
    "        \n",
    "        # create the up projection matrix and initialize to zero\n",
    "        self.up_proj = nn.Linear(r, out_dim, bias=False)\n",
    "        #nn.init.kaiming_uniform_(self.up_proj.weight, a=math.sqrt(5))\n",
    "\n",
    "        # Add the custom DiagonalLinear layer\n",
    "        self.diagonal_linear_b = nn.Parameter(torch.zeros(out_dim), requires_grad=True)\n",
    "        #nn.init.constant_(self.diagonal_linear_b, 0.01)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        pretrained_out = self.pretrained(x)\n",
    "\n",
    "        down_project_out = self.down_proj(x)\n",
    "\n",
    "        B, S, C = down_project_out.shape\n",
    "\n",
    "        q, k, v = self.Wqkv(down_project_out).reshape(B, S, 3, C//4).unbind(dim=2)\n",
    "        \n",
    "        mini_attn_output = q @ k.transpose(-2, -1)\n",
    "        mini_attn_output = mini_attn_output / math.sqrt(k.size(-1))\n",
    "\n",
    "        mini_attn_output = mini_attn_output.softmax(dim=-1)\n",
    "\n",
    "        mini_attn_output = mini_attn_output @ v\n",
    "\n",
    "        mini_attn_output = self.Wo(mini_attn_output)\n",
    "\n",
    "        up_project_out = self.up_proj(mini_attn_output)\n",
    "\n",
    "        diagonal_b_out = up_project_out * self.diagonal_linear_b\n",
    "\n",
    "        return pretrained_out + diagonal_b_out\n",
    "\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"Wqkv\" not in name and \"Wo\" not in name and \"diagonal_linear_b\" not in name and \"classifier\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def create_peft(module):\n",
    "    \"\"\"Converts a linear module to a peft linear module.\"\"\"\n",
    "    k, d = module.weight.shape  # pytorch nn.Linear weights are transposed, that is why shape is (k, d) and not (d, k)\n",
    "    peft = SHA_DIAGONAL(in_dim=d, out_dim=k)\n",
    "    with torch.no_grad():\n",
    "        peft.pretrained.weight.copy_(module.weight)\n",
    "        peft.pretrained.bias.copy_(module.bias)\n",
    "    return peft   \n",
    "\n",
    "\n",
    "\n",
    "def add_peft_layers(\n",
    "    model,\n",
    "    module_names: Tuple=(\"query\", \"value\"),\n",
    "    ignore_layers: List[int]=[]\n",
    "):\n",
    "    module_types: Tuple=(nn.Linear,)\n",
    "\n",
    "    # disable dropout in frozen layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = 0.0\n",
    "    # replace chosen linear modules with lora modules\n",
    "    model_name\n",
    "        if isinstance(module, module_types) and name in module_names:\n",
    "            temp_peft = create_peft(module)\n",
    "            setattr(model, name, temp_peft)\n",
    "        else:\n",
    "            ignore_layers_str = [str(i) for i in ignore_layers]\n",
    "            if name not in ignore_layers_str:\n",
    "                add_peft_layers(module, module_names, ignore_layers)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SHA_Diagonal/engine.py\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import config\n",
    "import peft_module\n",
    "\n",
    "\n",
    "def eval_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return accuracy_score(labels, preds_flat)    \n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "\n",
    "        inputs = {'input_ids':      batch['input_ids'].to(config.CONFIG.device),\n",
    "                  'attention_mask': batch['attention_mask'].to(config.CONFIG.device),\n",
    "                  'labels':         batch['labels'].to(config.CONFIG.device),\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[\"loss\"]\n",
    "        logits = outputs[\"logits\"]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(val_dataloader)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader):\n",
    "\n",
    "    epochs = config.CONFIG.epochs\n",
    "    model.to(config.CONFIG.device)\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "\n",
    "      model.train()\n",
    "\n",
    "      loss_train_total = 0\n",
    "\n",
    "      progress_bar = tqdm(train_dataloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=True)\n",
    "\n",
    "      for batch in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = {'input_ids':      batch['input_ids'].to(config.CONFIG.device),\n",
    "                  'attention_mask': batch['attention_mask'].to(config.CONFIG.device),\n",
    "                  'labels':         batch['labels'].to(config.CONFIG.device),\n",
    "                }\n",
    "\n",
    "        output = model(**inputs)\n",
    "\n",
    "        loss = output[\"loss\"]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "\n",
    "      tqdm.write(f'\\nEpoch {epoch}')\n",
    "      loss_train_avg = loss_train_total/len(train_dataloader)\n",
    "      tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "\n",
    "      val_loss, predictions, true_vals = evaluate(model, val_dataloader)\n",
    "      val_f1 = eval_func(predictions, true_vals)\n",
    "      tqdm.write(f'Validation loss: {val_loss}')\n",
    "      tqdm.write(f'Accuracy : {val_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SHA_Diagonal/train.py\n",
    "\n",
    "import config, GLUE_data_setup, peft_module, engine\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "task = config.CONFIG.task\n",
    "train_dataset = GLUE_data_setup.GLUEDataset(dataset_name=config.CONFIG.task, split = \"train\")\n",
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "validation_dataset = GLUE_data_setup.GLUEDataset(dataset_name=config.CONFIG.task, split = validation_key)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.CONFIG.train_batch,\n",
    "                              num_workers=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=config.CONFIG.valid_batch,\n",
    "                              num_workers=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "torch.manual_seed(config.CONFIG.seed)\n",
    "torch.cuda.manual_seed(config.CONFIG.seed)\n",
    "num_labels = 3 if config.CONFIG.task.startswith(\"mnli\") else 1 if config.CONFIG.task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.CONFIG.model_name, num_labels = num_labels, output_attentions = False,\n",
    "                                                           output_hidden_states = False).to(config.CONFIG.device)\n",
    "\n",
    "peft_module.add_peft_layers(model=model) \n",
    "peft_module.freeze_model(model)\n",
    "\n",
    "\n",
    "# Identify classifier parameters and other parameters\n",
    "classifier_parameters = [p for n, p in model.named_parameters() if \"classifier\" in n and p.requires_grad]\n",
    "other_parameters = [p for n, p in model.named_parameters() if \"classifier\" not in n and p.requires_grad]\n",
    "\n",
    "# Create optimizer with parameter groups\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': other_parameters, 'lr': config.CONFIG.learning_rate},\n",
    "    {'params': classifier_parameters, 'lr': config.CONFIG.classifier_learning_rate}\n",
    "])\n",
    "\n",
    "# Define the total number of training steps and warm-up steps\n",
    "total_steps = len(train_loader) * config.CONFIG.epochs\n",
    "warmup_steps = int(config.CONFIG.warmup_ratio * total_steps)\n",
    "\n",
    "# Create the scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "engine.train(model=model, optimizer=optimizer, scheduler=scheduler, train_dataloader=train_loader, val_dataloader=validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/azimi/SHA_Diagonal/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/azimi/SHA_Diagonal/utils.py\n",
    "def num_parameters(model):\n",
    "    total_params_1 = 0\n",
    "    total_params_2 = 0\n",
    "    for param_name, weights in model.named_parameters():\n",
    "      if weights.requires_grad == True:\n",
    "        total_params_1 += weights.numel()\n",
    "\n",
    "    for param_name, weights in model.named_parameters():\n",
    "      if 'classifier' in param_name:\n",
    "        total_params_2 += weights.numel()\n",
    "\n",
    "    print(\"total_params:\", total_params_1-total_params_2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
